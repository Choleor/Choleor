# ML 비지도 학습으로 음악 간의 유사도 비교
현재 진행중인 '안무 생성 서비스'에서 필자가 맡았던 주요 로직과 그 로직에 들어가는 기술에 대해 포스팅하려 한다.

안무 생성에 사용되는 DB는 안무가를 불러 직접 촬영한 영상이 아닌 youtube 영상과 해당 안무에 들어가는 음원을 기준으로 하였다.

일단 간단하게 안무를 만드는 플로우를 살펴보면,

1. 사용자가 안무를 만들고 싶어하는 노래를 입력받으면 박자 단위로 구간을 자른다.

2. 자른 음원을 바탕으로 기존 시스템에 있는 음원 DB들과 함께 비지도학습으로 유사한 음원끼리 묶는다.

3. 자른 음원과 유사한 음원들 중 가장 유사한 음원 n개를 추출한다.

4. 안무 동작은 frame 단위로 정밀하게 잘라서 각 동작 별로 pose estimation 진행 후 동작간 조화도를 수치화하여 어울리는 동작 n개를 추출한다.

5. 동작의 크기와 노래의 진폭을 비교해서 이를 점수화하여 노래와 안무가 서로 조화로운지 비교한다.

6. 3,4,5 과정을 바탕으로 최종 안무 후보 6개를 도출한다.

7. 각 구간별로 후보 6개 중 하나씩 동작을 선택하고, 선택이 끝나면 교차편집하여 최종 안무 영상을 제공한다.

이렇게 나눌 수 있다. 이 중에서 2,3번과 5번의 노래의 진폭을 점수화했던 내용을 담당하여 관련 내용을 기록하고자 한다.

### 비지도학습을 이용하여 유사한 음원끼리 묶기

음악이 서로 유사한지 비교하려면 어떤 값을 기준으로 해야할까? 가장 많이 쓰이는 방법은 음악의 feature를 분석해서 feature를 숫자로 나타낸 데이터값을 기반으로 여러 음원 파일들을 학습하여 비교하는 것이 있다. 즉, feature를 추출해야하는데, 종류도 여러가지가 있다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdazdqo%2FbtqNx8JBaKy%2F3kLpk7jp3ZrKcTEeePZvm0%2Fimg.jpg)

(사진 출처: ' Music similarity-based approach to generating dance  motion sequence' 논문의 일부분)


Chromagram,MFCC를 비롯하여 수많은 feature가 있고 각각의 feature마다 n개의 차원이 존재한다.

그래서 처음에는 이 많은 값들을 한번에 normalize하여 비교하는데에 어려움이 있었다. 음악적인 지식이 있긴 했지만 기본적인 박자,조성과 같은 내용만 알고 있었고 전문적인 경험과 지식은 전무했기 때문에 어떤 feature를 중요하게 여겨야 하는지 전혀 알지 못했다. 지도학습이 아닌 비지도학습(unsupervised learning)을 선택한 이유도 이 때문이었다.

그래서 위 논문뿐만 아니라 다른 논문, 자료들을 찾아보았지만 대부분 비슷한 내용이었고 feature extraction을 바탕으로 머신러닝을 진행하는 프로젝트는 대부분 사람의 음성을 인식하여 응용하는 프로젝트가 많았다.

우리는 주변 소리, 사람의 목소리가 아닌 믹싱된 음원의 feature를 분석하는 것이 목적이었기 때문에 제일 중요시 여겼던 요소인 박자, 멜로디 등을 고려하여 chromagram과 mfcc를 위주로 extraction을 해봤다.

 

라이브러리마다 mfcc는 차원의 수가 달랐는데 정확한 이유는 분석하지 못하였다. librosa 라이브러리에서는 무려 20차원까지 있었고 mfcc의 n번째 차원마다 또 여러 개의 데이터값이 존재했다. 즉 20*n개의 차원으로 이루어진 것이다. 이 많은 값들을 줄여서 normalize한 뒤 비슷한 노래끼리 묶기 위해 차원 축소 방법을 고민하다가 가장 많이 알려진 pca(주성분 분석) 방법을 선택하게 되었다.

 

실제 코드는 다음과 같이 작성했다. (빠진 라이브러리 import문이 있을 수도 있다.. 그리고 대부분 csv 파일 변환을 위해 pandas를 자주 활용하였다)

구간 파일 하나마다 feature 분석후 pca로 차원을 줄이는 과정이다.


```
import librosa.display,librosa
from sklearn.decomposition import PCA
import pandas as pd
def getFeatureFile(filePath, file):
    X = pd.DataFrame()
    columns = []
    for i in range(20):
        columns.append("mfcc" + str(i + 1))

    music, fs = librosa.load(filePath + file)
    mfcc_music = librosa.feature.mfcc(music, sr=fs)

    pca = PCA(n_components=1, whiten=True)  # use pca
    X_pca = pca.fit_transform(mfcc_music)
    fin_pca = []

    for index in range(len(X_pca)):
        fin_pca.append(X_pca[index, 0])
    df_pca = pd.Series(fin_pca)

    X = pd.concat([X, df_pca], axis=1)

    data = X.T.copy()
    data.columns = columns
    data.index = [file]

    return data
```

feature extraction 값을 csv로 export하면 결과는 다음과 같이 나온다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9rXek%2FbtqNqeY63B4%2FoGA4kz5wxwbajsvW2ihTM1%2Fimg.png)

실제 DB에 있는 노래 중 일부만 보여준 모습이다. 

이후 비슷한 음원끼리 묶는 과정은 kmeans clustering으로 진행하였다. 처음에는 딥러닝 CNN으로 해볼까하는 아이디어가 있었는데, 데이터셋 수와 메모리의 한계가 있었다. 학습을 위해서는 음악 feature 중 스펙트로그램을 이미지로 시각화한 많은 데이터가 필요한데 이를 확보하기가 어려웠고 데이터 확장(augmentation)이 필요하기 때문에 노래를 구간마다 자르는 것이 전부인 전처리 방식으로는 머신러닝이 적합하다고 판단하였다.

또한 스펙트로그램을 이미지로 저장할 때 많은 용량의 메모리를 요구하는데 조사해본 결과 CNN으로 음악 장르를 classification했을 때 필요한 메모리에 비해 정확도가 낮은 편이었기 때문에(epoch:50, accuracy:약 33%) 머신러닝으로 방향을 정했다.

kmeans clustering은 비지도학습으로 이루어지며, 따로 학습 데이터를 확보할 필요없이 매번 실행할 때마다 데이터를 넣어주면 되는데, 앞서 차원을 대폭 줄인 array형태의 데이터값을 input으로 하므로 클러스터링할 때 그리 오랜 시간이 걸리지 않았다. (몇 초면 끝난다..!)

오디오 데이터를 일괄적으로 라벨링하기 어렵고 하나의 오디오 파일에서도 수많은 피처가 있기 때문에 지도 학습으로는 어렵다고 보았다. 따라서 데이터 간의 상관관계를 기계가 학습할 수 있도록 비지도 학습으로 진행하며 군집에서 중심점(Centroid)을 계속 찾아나가는 KMeans를 이용하였고 이 때 python sklearn 라이브러리를 이용하여 구현하였다.

 

실제 구현한 함수 로직을 보면 다음과 같다.

```
from sklearn.cluster import KMeans
# KMeans Clustering(n_clusters가 군집 개수)
    n_clusters = 10
    model = KMeans(n_clusters)
    model.fit(feat_data)
    labels_data = model.predict(feat_data)
```

실제 코드는 몇줄 되지 않는다..! 여기서 input으로 들어가는 feat_data가 바로 앞에 나왔던 csv 파일이다

우리 프로젝트에서 사용되는 음악 파일(한 파일당 약 60초 정도의 길이라 가정)은 약 200개 정도이며, 이를 다시 나누면 약 60/4=15개의 구간이 생성된다. 따라서 200*15=3,000여개의 노래 파일의 feature를 바탕으로 clustering을 하게 된다. 

kmeans에서 중요한 요소 중에 하나가 군집의 개수인데 군집 1개부터 10개까지 성능을 확인할 수 있는 지표인 inertia를 확인해보면 다음과 같다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYyBGH%2FbtqNx83Wcg1%2FjKahtXcNgiKyz8JAlfGPNK%2Fimg.png)

기울기가 급격한 부분이 가장 적합한 군집 수인데 여기서는 뚜렷한 숫자가 보이지 않는다고 판단했다. 제일 가파른 부분이 2,3 정도 되는데 군집이 2~3개밖에 되지 않으면 유의미한 결과가 나타나지 않아 10개로 설정했다.

 
그리고 실제로 클러스터링을 돌리면 학습 데이터가 정해져있지 않고 그때그때 들어오는 데이터를 클러스터링하기 때문에 결과가 다소 차이가 나며, 라벨은 0~9의 숫자로 랜덤으로 정해진다.

### 가장 유사한 음원 n개 추출하기

같은 군집 내에서 유사한 음원 파일을 찾기 위해 코사인 거리 측정법을 이용했다.

하나의 음원파일 feature 데이터에 20개 요소를 가지기 때문에 이때 한번 더 reshape로 차원을 축소시켰고

출력 형태는 파일명을 key, 유사도 점수(~100점 까지 표현)를 value로 하여 dictionary 타입으로 설정했다.

```
def cos_sim(A, B):
    return dot(A, B) / (norm(A) * norm(B))

def cluster_sim(data,idx,n):
    sim_dic = {}

    for i in range(len(data.values)):
        seg1 = data.values[idx]  # 비교할 audio_slice
        seg2 = data.values[i]  # 같은 cluster에 있는 audio_slice 여러개

        seg1_2d = seg1.reshape(-1, 1)  # 차원 축소
        seg2_2d = seg2.reshape(-1, 1)

        sim = cos_sim(np.squeeze(seg1_2d), np.squeeze(seg2_2d))
        sim = sim * 100  # 퍼센트(%) 단위로 나타냄
        sim = round(sim, 2)  # 소수 둘째자리에서 반올림

        audio_slice_id = data.index[i]

        sim_dic[audio_slice_id] = sim

    final_dic = sorted(sim_dic.items(), reverse=True, key=lambda x: x[1])  # 내림차순 정렬
    return final_dic[1:n]
```

결과 예시는 [('파일명1',99.6),('파일명2',98.1),...]와 같은 형태를 가지며 실제 코드에서는 제일 유사한 파일 상위 6개까지 출력되도록 파라미터를 수정했다.

### 노래의 진폭을 바탕으로 노래-안무간의 조화도 비교 

노래의 진폭은 python librosa 라이브러리를 사용하였고, 진폭 단위는 데시벨을 기준으로 했다.

librosa에서 측정하는 raw 진폭값은 음수값부터 시작해서 범주가 맞지 않기 때문에 사람이 들을 수 있는 0데시벨부터 다시 데이터 정규화를 진행했다. 그리고 진폭은 구간 전체로 계산하지 않고 박자별로 다시 나누어서 한 구간 안에서도 음악의 진폭(노래 크기)이 어떻게 변하는지 알아볼 수 있도록 하였다.

그래서 코드에서 보면 8박자 기준으로 나누었고 박자의 변화가 8개가 생성되어야 하므로 구간은 9개로 설정해서 9로 나누었다.


그리고 노래-안무간의 조화도를 비교할때 범주형으로 들어가기보다 단계별로 계산하면 더 효율적일 것으로 보아 

전체 0~30dB에서 3데시벨씩 단계를 나누어 최종 return 타입은 int 단계값이 되도록 코드를 구현했다.

```
    music, fs = librosa.load(file)
    music_stft = librosa.stft(music)
    music_amp = librosa.amplitude_to_db(abs(music_stft))
    music_fin_amp = [[0 for i in range(len(music_amp[j]))] for j in range(len(music_amp))]

    for i in range(len(music_amp)):
        for j in range(len(music_amp[i])):
            if (music_amp[i][j] >= 0):
                music_fin_amp[i][j] = music_amp[i][j]

    length = len(music_fin_amp)  # dB array 길이: 1025
    len_unit = int(length / 9)  # 8박자 단위로 자르기 때문에 9로 나누기
    amp_list = []
```

위 코드는 amplitude 단계를 출력해주는 코드의 일부분이다.

 

결국 음악의 유사성과 노래 진폭 연산이 가능하도록 백엔드 로직을 만들었고 이 과정이 끝난 후 동작간의 연결성을 고려해서 최종적으로 노래에 맞는 다른 안무 동작 후보가 생성된다.
